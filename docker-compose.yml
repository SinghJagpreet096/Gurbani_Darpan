version: "3.8"

services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama:/app/ollama
    entrypoint: >
      sh -c "
        ollama serve & 
        sleep 5 && 
        llama run hf.co/singhjagpreet/llama3.1_8b-Gurmukhi_v2 && 
        ollama create gurbani -f /app/ollama/Modelfile &&
        tail -f /dev/null
      "
  backend:
    build: ./backend
    ports:
      - "5001:5001"
    depends_on:
      - ollama  # Ensure ollama starts before backend

  frontend:
    image: nginx
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
    ports:
      - "8000:80"

volumes:
  ollama_data:  # Explicitly declare the volume
